# -*- coding: utf-8 -*-
"""ROI_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18lo40YP9X6QRaOfbUcGcJmc-qHP41IvK
"""

import pandas as pd

# Replace 'your_dataset.csv' with the actual path to your CSV file
merged_roi_df = pd.read_csv('/content/datasets/final_roi_merged.csv')

# Display the first few rows of the DataFrame
merged_roi_df.head()

import matplotlib.pyplot as plt

# Choose top N keywords by frequency or total revenue
top_keywords = merged_roi_df['clean_keyword'].value_counts().head(5).index.tolist()

# Filter dataset for just those keywords
top_keywords_df = merged_roi_df[merged_roi_df['clean_keyword'].isin(top_keywords)]

# Plot
plt.figure(figsize=(12, 6))
for kw in top_keywords:
    subset = top_keywords_df[top_keywords_df['clean_keyword'] == kw]
    plt.plot(subset['period'], subset['ROI'], marker='o', label=kw)

plt.title('üìà ROI Trends Over Time for Top Keywords')
plt.xlabel('Period')
plt.ylabel('ROI')
plt.legend(title='Keyword')
plt.grid(True)
plt.tight_layout()
plt.show()

# Get total revenue and cost by keyword
agg = merged_roi_df.groupby('clean_keyword').agg({
    'Paid_Cost': 'sum',
    'revenue': 'sum'
})
agg['ROI'] = agg['revenue'] / (agg['Paid_Cost'] + 1e-6)

# Top 10 by ROI
top10 = agg.sort_values(by='ROI', ascending=False).head(10)

# Plot
top10['ROI'].plot(kind='barh', figsize=(10, 6), color='green')
plt.title('üí∏ Top 10 Keywords by ROI (Overall)')
plt.xlabel('ROI')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Filter rows where revenue is zero
zero_revenue_keywords = merged_roi_df[merged_roi_df['revenue'] == 0]

# Step 2: Group by keyword + period and sum Paid_Cost
zero_revenue_per_period = zero_revenue_keywords.groupby(
    ['clean_keyword', 'period']
)['Paid_Cost'].sum().reset_index()

# Step 3: Sort and get top 10 keywords by wasted spend
top_zero_costs = zero_revenue_per_period.sort_values(by='Paid_Cost', ascending=False).head(10)

# Step 4: Plot
plt.figure(figsize=(12, 6))
plt.barh(
    top_zero_costs['clean_keyword'] + " (P" + top_zero_costs['period'].astype(str) + ")",
    top_zero_costs['Paid_Cost'],
    color='crimson'
)
plt.title("üõë Top 10 Keywords With $0 Revenue by Period")
plt.xlabel("Total Paid Cost ($)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Group revenue by keyword and period
keyword_revenue_by_period = merged_roi_df.groupby(
    ['clean_keyword', 'period']
)['revenue'].sum().reset_index()

# Step 2: Select the top keyword for each period
best_keywords_per_period = keyword_revenue_by_period.loc[
    keyword_revenue_by_period.groupby('period')['revenue'].idxmax()
].sort_values(by='period').reset_index(drop=True)

# Step 3: Create bar chart
plt.figure(figsize=(12, 6))
plt.bar(
    best_keywords_per_period['period'].astype(str) + " - " + best_keywords_per_period['clean_keyword'],
    best_keywords_per_period['revenue'],
    color='royalblue'
)
plt.xticks(rotation=45, ha='right')
plt.title("üí∞ Best Revenue-Generating Keyword in Each Period")
plt.ylabel("Total Revenue ($)")
plt.xlabel("Period - Keyword")
plt.tight_layout()
plt.show()

# Step 1: Group by keyword + period and sum revenue
keyword_revenue_by_period = merged_roi_df.groupby(
    ['clean_keyword', 'period']
)['revenue'].sum().reset_index()

# Step 2: Exclude 'labelmaster' and 'labelmastercom'
filtered_keywords = keyword_revenue_by_period[
    ~keyword_revenue_by_period['clean_keyword'].isin(['labelmaster', 'labelmastercom'])
]

# Step 3: Find the top keyword in each period
top_keyword_excluding_labelmaster = filtered_keywords.loc[
    filtered_keywords.groupby('period')['revenue'].idxmax()
].sort_values(by='period').reset_index(drop=True)

# Preview
print(top_keyword_excluding_labelmaster)

import matplotlib.pyplot as plt

# Plot the top keyword per period (excluding labelmaster variants)
plt.figure(figsize=(12, 6))
plt.bar(
    top_keyword_excluding_labelmaster['period'].astype(str) + " - " + top_keyword_excluding_labelmaster['clean_keyword'],
    top_keyword_excluding_labelmaster['revenue'],
    color='dodgerblue'
)

plt.xticks(rotation=45, ha='right')
plt.title("üí∞ Top Revenue-Generating Keyword in Each Period (Excluding 'labelmaster')")
plt.ylabel("Total Revenue ($)")
plt.xlabel("Period - Keyword")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Group and calculate ROI (if not already done)
keyword_roi = merged_roi_df.groupby('clean_keyword').agg({
    'Paid_Cost': 'sum',
    'revenue': 'sum'
}).reset_index()

keyword_roi['ROI'] = keyword_roi['revenue'] / (keyword_roi['Paid_Cost'] + 1e-6)

# Step 2: Filter keywords with ROI ‚â• 2
keep_paid_keywords = keyword_roi[keyword_roi['ROI'] >= 2].sort_values(by='ROI', ascending=False).head(15)

# Step 3: Plot
plt.figure(figsize=(12, 6))
plt.barh(keep_paid_keywords['clean_keyword'], keep_paid_keywords['ROI'], color='seagreen')
plt.title("‚úÖ Keywords to Keep Paying (ROI ‚â• 2)")
plt.xlabel("ROI")
plt.ylabel("Keyword")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

keyword_roi['ROI'] = keyword_roi['revenue'] / (keyword_roi['Paid_Cost'] + 1e-6)

# Step 2: Filter for ROI ‚â• 2 and exclude labelmaster variants
keep_paid_keywords = keyword_roi[
    (keyword_roi['ROI'] >= 2) &
    (~keyword_roi['clean_keyword'].isin(['labelmaster', 'labelmastercom']))
].sort_values(by='ROI', ascending=False).head(15)

# Step 3: Plot
plt.figure(figsize=(12, 6))
plt.barh(keep_paid_keywords['clean_keyword'], keep_paid_keywords['ROI'], color='seagreen')
plt.title("‚úÖ Keywords to Keep Paying (ROI ‚â• 2, Excluding 'labelmaster')")
plt.xlabel("ROI")
plt.ylabel("Keyword")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Step 1: Create a classification-specific DataFrame
df_cls = merged_roi_df.copy()

# Step 2: Create binary label: 1 = Keep Paying (ROI ‚â• 2), 0 = Cut Paid
df_cls['label'] = df_cls['ROI'].apply(lambda x: 1 if x >= 2 else 0)

# Step 3: Select features and target
X = df_cls[['Paid_Cost', 'revenue', 'period']]
y = df_cls['label']

# Step 4: Check class distribution
print("Label Distribution:\n", y.value_counts())
print("\nFeature Sample:\n", X.head())

# Step 1: Create new classification DataFrame
df_cls = merged_roi_df.copy()

# Step 2: Update label logic: 1 = Keep Paying (ROI ‚â• 1.5), else 0 = Cut Paid
df_cls['label'] = df_cls['ROI'].apply(lambda x: 1 if x >= 1.5 else 0)

# Step 3: Select features and target
X = df_cls[['Paid_Cost', 'revenue', 'period']]
y = df_cls['label']

# Step 4: Check class distribution
print("Updated Label Distribution:\n", y.value_counts())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Confirm shape
print("Training shape:", X_train_scaled.shape)
print("Test shape:", X_test_scaled.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Train the model
clf = LogisticRegression(random_state=42)
clf.fit(X_train_scaled, y_train)

# Predict on test set
y_pred = clf.predict(X_test_scaled)

# Evaluate
print("üîç Accuracy:", accuracy_score(y_test, y_pred))
print("\nüìâ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nüìã Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Train the model with balanced class weights
clf_balanced = LogisticRegression(class_weight='balanced', random_state=42)
clf_balanced.fit(X_train_scaled, y_train)

# Predict on test set
y_pred_balanced = clf_balanced.predict(X_test_scaled)

# Evaluate
print("üîç Accuracy (balanced):", accuracy_score(y_test, y_pred_balanced))
print("\nüìâ Confusion Matrix:\n", confusion_matrix(y_test, y_pred_balanced))
print("\nüìã Classification Report:\n", classification_report(y_test, y_pred_balanced))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Step 1: Train the model
rf_model = RandomForestClassifier(
    class_weight='balanced',
    n_estimators=100,
    max_depth=None,
    random_state=42
)
rf_model.fit(X_train_scaled, y_train)

# Step 2: Predict
y_pred_rf = rf_model.predict(X_test_scaled)

# Step 3: Evaluate
print("üîç Accuracy (Random Forest):", accuracy_score(y_test, y_pred_rf))
print("\nüìâ Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("\nüìã Classification Report:\n", classification_report(y_test, y_pred_rf))

# Step 1: Convert test set to DataFrame for joining
X_test_df = X_test.copy()
X_test_df['true_label'] = y_test.values
X_test_df['predicted_label'] = y_pred_rf

# Step 2: Merge back with keyword info (clean_keyword + ROI)
test_with_keywords = X_test_df.copy()
test_with_keywords = test_with_keywords.merge(
    df_cls[['Paid_Cost', 'revenue', 'period', 'clean_keyword', 'ROI']],
    on=['Paid_Cost', 'revenue', 'period'],
    how='left'
)

# Step 3: Filter False Negatives (actual = 1, predicted = 0)
false_negatives = test_with_keywords[
    (test_with_keywords['true_label'] == 1) &
    (test_with_keywords['predicted_label'] == 0)
]

# Step 4: View result
print("‚ùå False Negative Keywords (missed Keep Paying):\n")
print(false_negatives[['clean_keyword', 'Paid_Cost', 'revenue', 'ROI', 'period']])

pip install xgboost

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize model
xgb_model = XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    scale_pos_weight=2,   # handle class imbalance
    random_state=42
)

# Train the model
xgb_model.fit(X_train_scaled, y_train)

# Predict
y_pred_xgb = xgb_model.predict(X_test_scaled)

# Evaluate
print("üîç Accuracy (XGBoost):", accuracy_score(y_test, y_pred_xgb))
print("\nüìâ Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("\nüìã Classification Report:\n", classification_report(y_test, y_pred_xgb))

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize model
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

# Train the model
gb_model.fit(X_train_scaled, y_train)

# Predict
y_pred_gb = gb_model.predict(X_test_scaled)

# Evaluate
print("üîç Accuracy (Gradient Boosting):", accuracy_score(y_test, y_pred_gb))
print("\nüìâ Confusion Matrix:\n", confusion_matrix(y_test, y_pred_gb))
print("\nüìã Classification Report:\n", classification_report(y_test, y_pred_gb))

import joblib

joblib.dump(scaler, "scaler.pkl")

# Download to local
from google.colab import files

files.download("scaler.pkl")

# Visualize ROI distribution
import matplotlib.pyplot as plt
merged_roi_df['ROI'].hist(bins=30)
plt.axvline(1.2, color='orange', linestyle='--', label='1.2 Threshold')
plt.axvline(1.5, color='green', linestyle='--', label='1.5 Threshold')
plt.title("ROI Distribution")
plt.xlabel("ROI")
plt.ylabel("Keyword Count")
plt.legend()
plt.show()

# Create new classification DataFrame
df_cls_1_2 = merged_roi_df.copy()

# Update label: 1 = Keep Paying (ROI ‚â• 1.2), 0 = Cut Paid
df_cls_1_2['label'] = df_cls_1_2['ROI'].apply(lambda x: 1 if x >= 1.2 else 0)

# Features and target
X = df_cls_1_2[['Paid_Cost', 'revenue', 'period']]
y = df_cls_1_2['label']

# Check class distribution
print("Updated Label Distribution:\n", y.value_counts())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Scale the features
scaler_1_2 = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Train set size:", X_train.shape)
print("Test set size:", X_test.shape)

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize XGBoost Classifier
xgb_model_1_2 = XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    scale_pos_weight=(69 / 45),  # Balance the slightly uneven class
    random_state=42
)

# Train the model
xgb_model_1_2.fit(X_train_scaled, y_train)

# Predict on test set
y_pred_xgb_1_2 = xgb_model_1_2.predict(X_test_scaled)

# Evaluate
print("üîç Accuracy (XGBoost with ROI ‚â• 1.2):", accuracy_score(y_test, y_pred_xgb_1_2))
print("\nüìâ Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb_1_2))
print("\nüìã Classification Report:\n", classification_report(y_test, y_pred_xgb_1_2))

from sklearn.linear_model import LogisticRegression

clf_balanced_1_2 = LogisticRegression(class_weight='balanced', random_state=42)
clf_balanced_1_2.fit(X_train_scaled, y_train)

y_pred_clf = clf_balanced_1_2.predict(X_test_scaled)

print("üîç Accuracy (Logistic):", accuracy_score(y_test, y_pred_clf))
print(confusion_matrix(y_test, y_pred_clf))
print(classification_report(y_test, y_pred_clf))

from sklearn.ensemble import RandomForestClassifier

rf_model_1_2 = RandomForestClassifier(class_weight='balanced', random_state=42)
rf_model_1_2.fit(X_train_scaled, y_train)

y_pred_rf = rf_model_1_2.predict(X_test_scaled)

print("üîç Accuracy (Random Forest):", accuracy_score(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

from sklearn.ensemble import GradientBoostingClassifier

gb_model_1_2 = GradientBoostingClassifier(random_state=42)
gb_model_1_2.fit(X_train_scaled, y_train)

y_pred_gb = gb_model_1_2.predict(X_test_scaled)

print("üîç Accuracy (Gradient Boosting):", accuracy_score(y_test, y_pred_gb))
print(confusion_matrix(y_test, y_pred_gb))
print(classification_report(y_test, y_pred_gb))

joblib.dump(xgb_model_1_2, "xgb_model_1_2.pkl")
files.download("xgb_model_1_2.pkl")

joblib.dump(scaler, "scaler_1_2.pkl")
files.download("scaler_1_2.pkl")

# Step 1: Prepare feature matrix
X_full = merged_roi_df[['Paid_Cost', 'revenue', 'period']]

# Step 2: Scale using both scalers
X_scaled_1_5 = scaler.transform(X_full)  # from ROI ‚â• 1.5 model
X_scaled_1_2 = scaler_1_2.transform(X_full)  # from ROI ‚â• 1.2 model

# Step 3: Get predictions from both models
pred_1_5 = xgb_model.predict(X_scaled_1_5)
pred_1_2 = xgb_model_1_2.predict(X_scaled_1_2)

# Step 4: Find common "Keep Paying" predictions
keep_in_both = (pred_1_5 == 1) & (pred_1_2 == 1)

# Step 5: Extract keywords
common_keywords = merged_roi_df[keep_in_both]['clean_keyword'].unique()

# Display results
print("‚úÖ Keywords predicted as 'Keep Paying' in both models:", len(common_keywords))
print(common_keywords[:20])  # show top 20